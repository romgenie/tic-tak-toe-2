{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Overview","text":"<p>Goals: exhaustive, deterministic, research-grade tic-tac-toe solver and datasets.</p> <p>Design highlights: canonicalization by dihedral group, strict determinism for reproducibility, CLI for exporting datasets.</p> <p>See other pages for details.</p>"},{"location":"benchmarks/","title":"Benchmarks","text":"<p>This page documents typical performance for solver and export operations and how to reproduce locally.</p>"},{"location":"benchmarks/#methodology","title":"Methodology","text":"<ul> <li>Hardware: note CPU model and RAM. Example: Apple M2 Pro, 16GB RAM.</li> <li>OS: macOS 14.x or Ubuntu 22.04.</li> <li>Python: 3.11 or 3.12.</li> <li>NumPy: tested on 1.26.x and 2.x.</li> <li>Environment: <code>pip install .[dev]</code> (optionally add <code>.[parquet]</code> for Parquet runs).</li> </ul> <p>We use <code>pytest-benchmark</code> to time hot paths. Benchmarks are stable and avoid tiny timers via <code>--benchmark-min-time=0.1</code>.</p>"},{"location":"benchmarks/#reproduce-locally","title":"Reproduce locally","text":"<pre><code>pip install .[dev]\n# optional for parquet\npip install .[parquet]\n\npytest -q -k benchmark --benchmark-min-time=0.1\n</code></pre>"},{"location":"benchmarks/#sample-results-indicative","title":"Sample results (indicative)","text":"<ul> <li>Perfect-play solver (enumerate all reachable states, memoized): typically &lt;100 ms.</li> <li>Dataset export (CSV, canonical-only, no augmentation): &lt;500 ms.</li> <li>Dataset export (both CSV+Parquet): slightly higher due to serialization overhead.</li> </ul> <p><code>pytest-benchmark</code> output example:</p> <pre><code>--------------------------------------------------------------------------------------- benchmark: 2 tests --------------------------------------------------------------------------------------\nName (time in ms)                      Min       Max      Mean   StdDev    Median      IQR  Outliers  OPS (Kops/s)  Rounds  Iterations\n-----------------------------------------------------------------------------------------------------------------------------------------------------------\ntest_solver_full_enumeration        45.000    60.000    50.000     3.00    49.500    4.000       2;0           20.0      20           1\ntest_export_small_csv              180.000   260.000   200.000    15.00   198.000   20.000       1;1            5.0      10           1\n-----------------------------------------------------------------------------------------------------------------------------------------------------------\n</code></pre> <p>Numbers vary slightly by machine and Python/NumPy version but are consistently within the same order of magnitude.</p>"},{"location":"benchmarks/#notes","title":"Notes","text":"<ul> <li>All pipelines are pure Python with vectorized components where applicable.</li> <li>CSV order is deterministic, which can add minimal sorting cost but is essential for reproducibility.</li> </ul>"},{"location":"cli/","title":"CLI Guide","text":"<p>Examples:</p> <ul> <li>ttt symmetry --board 100020000</li> <li>ttt solve --board 100020000</li> <li>ttt datasets export --out data_raw --canonical-only --no-augmentation --epsilons 0.1 --format csv</li> <li>Streaming: echo \"100020000\" | ttt symmetry --stdin</li> </ul>"},{"location":"datasets/","title":"Datasets","text":"<p>This page serves as the dataset card for the exported Tic-tac-toe datasets.</p> <p>Motivation: Provide an exact, exhaustive reference dataset derived from a perfect-play solver, suitable for benchmarking learning methods, verifying invariants, and reproducing published results.</p> <p>Composition: Two tables are exported.</p> <ul> <li>ttt_states: one row per reachable state (optionally canonical-only). Includes solver value, plies-to-end, optimal move mask, symmetry info, and rich features.</li> <li>ttt_state_actions: one row per legal action in each non-terminal state; includes Q-values, DTT action-tier, optimality mask, and policy targets (uniform-optimal, soft by decision-time tier, and soft by Q).</li> </ul> <p>Generation: Rows are generated via the in-repo solver and feature pipeline. See Reproducibility for commands and stable checksums.</p> <p>Preprocessing: Only deterministic numerical transforms; no randomness. Optional symmetry augmentation can expand state-action coverage.</p> <p>Features: Positional control, threats, connectivity, pattern strength, phase, and per-cell open-line counts. Policies obey probability constraints and are zero outside legal moves.</p> <p>Potential biases: Tic-tac-toe is trivial; policy mass concentrates on optimal moves. Augmentation changes class balance across action rows.</p> <p>Licensing: MIT; see LICENSE.</p> <p>Intended uses: Teaching, benchmarking, verifying RL targets. Misuses: Treating counts as representative of stochastic games or non-perfect-play agents.</p> <p>Determinism &amp; Manifest: The manifest.json includes row counts, checksums, orbit_size_distribution, terminal_split, parquet_written, and schema hashes. The schemas are emitted under export/schema/*.schema.json.</p>"},{"location":"datasets/#parquet-outputs","title":"Parquet outputs","text":"<p>Parquet is written only when explicitly requested via <code>--format parquet</code> or <code>--format both</code> and when the optional dependencies are installed:</p> <ul> <li>Install extras: <code>pip install .[parquet]</code> (installs pandas and pyarrow).</li> <li>Example: <code>python -m tictactoe.cli datasets export --out data_raw/example --format both --canonical-only</code>.</li> </ul> <p>If the dependencies are missing and you request <code>--format parquet</code>, the command fails early with a clear error. If you request <code>--format both</code>, the export gracefully degrades to CSV-only, writes <code>manifest.json</code>, and sets <code>parquet_written=false</code>.</p>"},{"location":"datasets/#dataset-card","title":"Dataset Card","text":"<ul> <li>License: MIT (matches repository license).</li> <li>Intended use: benchmarking algorithms and verifying invariants; educational demos of perfect play and symmetry.</li> <li>Misuse: inferring properties of stochastic or non-perfect agents; overfitting to trivial game dynamics.</li> <li>Determinism: fully deterministic; re-running with identical args produces byte-identical CSVs; manifest contains schema hashes and checksums.</li> <li>Reproducibility: see <code>docs/repro.md</code> for exact commands and environment notes.</li> <li>Reachable counts: captured in <code>manifest.json</code> (<code>row_counts</code>, <code>terminal_split</code>, <code>orbit_size_distribution</code>).</li> <li>Signatures: per-file SHA-256 in <code>manifest.json/checksums</code>.</li> </ul>"},{"location":"repro/","title":"Reproducibility","text":"<p>This project aims to be fully reproducible end-to-end: solver, datasets, and checksums.</p>"},{"location":"repro/#quick-start","title":"Quick start","text":"<p>1) Create a Python 3.11+ environment and install the package in editable mode with dev extras. 2) Export the small canonical dataset (CSV) and verify integrity.</p> <p>Example using the included Makefile target:</p> <pre><code>make reproduce-small\n</code></pre> <p>This runs the CLI export with CSV outputs and then executes <code>scripts/verify_export.py</code> to check checksums, row counts, and schema hashes.</p>"},{"location":"repro/#manual-commands","title":"Manual commands","text":"<p>If you prefer to run the steps manually:</p> <pre><code>python -m tictactoe.cli datasets export --out data_clean/small --format csv --canonical-only\npython scripts/verify_export.py data_clean/small\n</code></pre> <p>Expected outputs (tree abbreviated):</p> <pre><code>data_clean/small/\n\u251c\u2500\u2500 manifest.json\n\u251c\u2500\u2500 schema/\n\u2502   \u251c\u2500\u2500 ttt_states.schema.json\n\u2502   \u2514\u2500\u2500 ttt_state_actions.schema.json\n\u251c\u2500\u2500 ttt_states.csv\n\u2514\u2500\u2500 ttt_state_actions.csv\n</code></pre> <p>The verify script will fail with a non-zero exit code on any mismatch.</p>"},{"location":"repro/#notes-on-parquet","title":"Notes on Parquet","text":"<p>Parquet export is optional and requires pandas + pyarrow. Install extras with:</p> <pre><code>pip install .[parquet]\n</code></pre> <p>Use <code>--format parquet</code> or <code>--format both</code> to write Parquet. If you request <code>--format parquet</code> without the dependencies installed, the CLI will raise a clear error. If you request <code>--format both</code> without the dependencies, the export will gracefully degrade to CSV-only and still write <code>manifest.json</code> with <code>parquet_written=false</code>.</p>"},{"location":"repro/#determinism","title":"Determinism","text":"<p>CSV rows are written in a deterministic order (sorted by state, then action) to ensure byte-for-byte reproducibility. The manifest records SHA-256 checksums for every exported artifact.</p>"},{"location":"repro/#environment-pinning","title":"Environment pinning","text":"<p>For strict archival, use <code>environment.yml</code> or <code>requirements.txt</code> along with Python 3.11/3.12. Our CI tests against multiple NumPy versions (1.26 and 2.x). For long-term reproduction, we recommend capturing the exact <code>pip freeze</code> (or conda env export) with the manifest.</p>"},{"location":"theory/","title":"Theory","text":""},{"location":"theory/#minimax-with-strict-tie-breaks","title":"Minimax with strict tie-breaks","text":"<p>We solve states from the perspective of the side-to-move using exact minimax with memoization. The value v \u2208 {+1, 0, \u22121} denotes win/draw/loss under perfect play. Among moves with equal v we break ties by distance-to-terminal (plies):</p> <ul> <li>Win \u227b Draw \u227b Loss</li> <li>Among wins/draws: prefer the shortest plies-to-end</li> <li>Among losses: prefer the longest plies-to-end (delay the loss)</li> </ul> <p>This induces a total preorder over legal actions. Our solver returns per-action q_values \u2208 {+1,0,\u22121} and dtt_action (distance-to-terminal after taking the action), and the set of optimal_moves obeying the policy above.</p>"},{"location":"theory/#symmetry-group-d4-and-canonicalization","title":"Symmetry group (D4) and canonicalization","text":"<p>Tic-tac-toe on a 3\u00d73 board admits the dihedral-4 (D4) symmetry group with 8 elements: {id, rot90, rot180, rot270, hflip, vflip, d1, d2}. We precompute index mappings for each transformation and use them to:</p> <ul> <li>Canonicalize states by selecting the lexicographically smallest image across all 8 transforms.</li> <li>Remap action indices under symmetry for augmentation and invariance checks.</li> </ul> <p>For a board b and transform g \u2208 D4, let g\u00b7b be the transformed board and g\u00b7a the transformed action index. The solver\u2019s value is invariant: V(g\u00b7b) = V(b); optimal moves map equivariantly: g\u00b7ArgMax(b) = ArgMax(g\u00b7b).</p>"},{"location":"theory/#complexity-and-state-graph","title":"Complexity and state graph","text":"<p>From the empty board, 5478 states are reachable (4520 nonterminal; terminal split: X=626, O=316, draw=16). This is a tiny graph and the exact backward induction + memoization completes in milliseconds on commodity hardware. Our features and datasets are O(#states) and fit comfortably in memory.</p>"},{"location":"api/tictactoe/","title":"API Reference: tictactoe","text":"<p>tictactoe package.</p> <p>Core algorithms, symmetry handling, datasets, and a simple CLI.</p> <p>Convenience imports are exposed for common workflows.</p>"},{"location":"api/tictactoe/#tictactoe.extract_board_features","title":"<code>extract_board_features(board, solved_map, lambda_temp=0.5, q_temp=1.0, epsilons=None, normalize_to_move=False)</code>","text":"<p>Extract features for a board.</p> <p>By default, features are computed on the board as given. If normalize_to_move=True, we remap pieces so the side-to-move becomes X=1. Note: Policies and q-values are derived from the original board's solver output. Normalization only swaps labels 1&lt;-&gt;2; legality and q-values align by construction because moves are on indices, not on piece IDs.</p> Source code in <code>src/tictactoe/orchestrator.py</code> <pre><code>def extract_board_features(\n    board: List[int],\n    solved_map: Dict[str, dict],\n    lambda_temp: float = 0.5,\n    q_temp: float = 1.0,\n    epsilons: Optional[List[float]] = None,\n    normalize_to_move: bool = False,\n) -&gt; Dict[str, Any]:\n    \"\"\"Extract features for a board.\n\n    By default, features are computed on the board as given.\n    If normalize_to_move=True, we remap pieces so the side-to-move becomes X=1.\n    Note: Policies and q-values are derived from the original board's solver\n    output. Normalization only swaps labels 1&lt;-&gt;2; legality and q-values align\n    by construction because moves are on indices, not on piece IDs.\n    \"\"\"\n    if epsilons is None:\n        epsilons = [0.1]\n    to_move = 1 if board.count(1) == board.count(2) else 2\n    if normalize_to_move and to_move == 2:\n        # swap X and O labels to make current player X\n        normalized_board = [0 if v == 0 else (1 if v == 2 else 2) for v in board]\n        current_player = 1\n    else:\n        normalized_board = board[:]\n        current_player = to_move\n    x_count, o_count = get_piece_counts(normalized_board)\n    winner_raw = get_winner(board)\n    winner_norm = get_winner(normalized_board)\n    key = serialize_board(board)\n    norm_key = serialize_board(normalized_board)\n    reachable = key in solved_map\n\n    if reachable:\n        sol = solved_map[key]\n        value_current = sol['value']\n        plies_to_end = sol['plies_to_end']\n        optimal_moves = set(sol['optimal_moves'])\n        qvals = list(sol['q_values'])\n        dtt_a = list(sol['dtt_action'])\n        policy_targets = build_policy_targets(normalized_board, sol, lambda_temp=lambda_temp, q_temp=q_temp)\n        pol_uniform = policy_targets['policy_optimal_uniform']\n        pol_soft = policy_targets['policy_soft_dtt']\n        pol_soft_q = policy_targets['policy_soft_q']\n        eps_policies = {}\n        for eps in epsilons:\n            tag = f\"{int(round(eps*100)):03d}\"\n            eps_policies[tag] = epsilon_policy_distribution(normalized_board, sol, eps)\n        pol_entropy = -sum(p * np.log(p + 1e-10) for p in pol_uniform if p &gt; 0)\n        pol_soft_dtt_entropy = -sum(p * np.log(p + 1e-10) for p in pol_soft if p &gt; 0)\n        child_tiers = {\n            'child_wins': sum(1 for v in qvals if v == +1),\n            'child_draws': sum(1 for v in qvals if v == 0),\n            'child_losses': sum(1 for v in qvals if v == -1),\n        }\n        difficulty = difficulty_score(sol)\n        # optional: compute reply branching factor as the average number of\n        # legal replies for the opponent after optimal moves. Keep lightweight\n        # and deterministic. If no optimal moves, 0.0. Terminal children\n        # contribute 0 by definition (no replies).\n        legal_reply_counts: List[int] = []\n        for mv in optimal_moves:\n            child = board[:]\n            child[mv] = (1 if board.count(1) == board.count(2) else 2)\n            if get_winner(child) != 0 or is_draw(child):\n                legal_reply_counts.append(0)\n            else:\n                legal_reply_counts.append(sum(1 for v in child if v == 0))\n        reply_branching = float(sum(legal_reply_counts) / len(legal_reply_counts)) if legal_reply_counts else 0.0\n    else:\n        value_current = None\n        plies_to_end = None\n        optimal_moves = set()\n        qvals = [None]*9\n        dtt_a = [None]*9\n        pol_uniform = [None]*9\n        pol_soft = [None]*9\n        pol_soft_q = [None]*9\n        eps_policies = {}\n        pol_entropy = None\n        pol_soft_dtt_entropy = None\n        child_tiers = {'child_wins': 0, 'child_draws': 0, 'child_losses': 0}\n        difficulty = 0.0\n        reply_branching = 0.0\n\n    sym = symmetry_info(board)\n    legal = [normalized_board[i] == 0 for i in range(9)]\n    best_mask = [(i in optimal_moves) for i in range(9)] if reachable else [False]*9\n    cell_pot = calculate_cell_line_potentials(normalized_board)\n\n    # Extended positional/strategic features (deterministic and cheap)\n    # Use the normalized_board for player-relative metrics\n    x_threats = calculate_line_threats(normalized_board, 1)\n    o_threats = calculate_line_threats(normalized_board, 2)\n    x_conn = calculate_connectivity(normalized_board, 1)\n    o_conn = calculate_connectivity(normalized_board, 2)\n    control = calculate_control_metrics(normalized_board)\n    x_patterns = calculate_pattern_strength(normalized_board, 1)\n    o_patterns = calculate_pattern_strength(normalized_board, 2)\n    game_phase = calculate_game_phase(normalized_board)\n    x_two_open = count_two_in_row_open(normalized_board, 1)\n    o_two_open = count_two_in_row_open(normalized_board, 2)\n\n    features: Dict[str, Any] = {\n        'board_state': key,\n        'normalized_board_state': norm_key,\n    'swapped_color': int(normalize_to_move and to_move == 2),\n        'x_count': x_count,\n        'o_count': o_count,\n        'empty_count': normalized_board.count(0),\n        'move_number': x_count + o_count,\n        'current_player': current_player,\n        'is_terminal': winner_raw != 0 or is_draw(board),\n        'winner': winner_raw,\n        'winner_normalized': winner_norm,\n        'is_draw': is_draw(board),\n        'is_valid': is_valid_state(board),\n        'reachable_from_start': reachable,\n        'canonical_form': sym['canonical_form'],\n        'canonical_op': sym['canonical_op'],\n        'orbit_size': sym['orbit_size'],\n        'horizontal_symmetric': sym['horizontal_symmetric'],\n        'vertical_symmetric': sym['vertical_symmetric'],\n        'diagonal_symmetric': sym['diagonal_symmetric'],\n        'rotational_symmetric': sym['rotational_symmetric'],\n        'any_symmetric': sym['any_symmetric'],\n        'orbit_index': sym['orbit_index'],\n        'value_current': value_current,\n        'plies_to_end': plies_to_end,\n        'optimal_moves_count': len(optimal_moves),\n        'optimal_policy_entropy': pol_entropy,\n        'policy_soft_dtt_entropy': pol_soft_dtt_entropy,\n        'policy_soft_q_entropy': (\n            -sum(p * np.log(p + 1e-10) for p in pol_soft_q if p &gt; 0) if reachable else None\n        ),\n    # Scalar difficulty per state; 0.0 when not reachable\n    'difficulty_score': difficulty,\n        'reply_branching_factor': reply_branching,\n        **child_tiers,\n    # control metrics (symmetric)\n    **control,\n    # player-relative threats/connectivity/patterns\n    'x_row_threats': x_threats['row_threats'],\n    'x_col_threats': x_threats['col_threats'],\n    'x_diag_threats': x_threats['diag_threats'],\n    'x_total_threats': x_threats['total_threats'],\n    'o_row_threats': o_threats['row_threats'],\n    'o_col_threats': o_threats['col_threats'],\n    'o_diag_threats': o_threats['diag_threats'],\n    'o_total_threats': o_threats['total_threats'],\n    'x_connected_pairs': x_conn['connected_pairs'],\n    'x_total_connections': x_conn['total_connections'],\n    'x_isolated_pieces': x_conn['isolated_pieces'],\n    'x_cluster_count': x_conn['cluster_count'],\n    'x_largest_cluster': x_conn['largest_cluster'],\n    'o_connected_pairs': o_conn['connected_pairs'],\n    'o_total_connections': o_conn['total_connections'],\n    'o_isolated_pieces': o_conn['isolated_pieces'],\n    'o_cluster_count': o_conn['cluster_count'],\n    'o_largest_cluster': o_conn['largest_cluster'],\n    'x_open_lines': x_patterns['open_lines'],\n    'x_semi_open_lines': x_patterns['semi_open_lines'],\n    'x_blocked_lines': x_patterns['blocked_lines'],\n    'x_potential_lines': x_patterns['potential_lines'],\n    'o_open_lines': o_patterns['open_lines'],\n    'o_semi_open_lines': o_patterns['semi_open_lines'],\n    'o_blocked_lines': o_patterns['blocked_lines'],\n    'o_potential_lines': o_patterns['potential_lines'],\n    'x_two_in_row_open': x_two_open,\n    'o_two_in_row_open': o_two_open,\n    'game_phase': game_phase,\n    }\n\n    for i in range(9):\n        features[f'cell_{i}'] = normalized_board[i]\n        features[f'legal_{i}'] = int(legal[i])\n        features[f'best_{i}'] = int(best_mask[i])\n        features[f'q_value_{i}'] = qvals[i]\n        features[f'dtt_action_{i}'] = dtt_a[i]\n        features[f'canonical_action_map_{i}'] = apply_action_transform(i, sym['canonical_op'])\n        features[f'policy_uniform_{i}'] = pol_uniform[i] if reachable else None\n        features[f'policy_soft_{i}'] = pol_soft[i] if reachable else None\n        features[f'policy_soft_q_{i}'] = pol_soft_q[i] if reachable else None\n        for tag, pol in (eps_policies.items() if reachable else []):\n            features[f'epsilon_policy_{tag}_{i}'] = pol[i]\n        features[f'x_cell_open_lines_{i}'] = cell_pot['x_cell_open_lines'][i]\n        features[f'o_cell_open_lines_{i}'] = cell_pot['o_cell_open_lines'][i]\n\n    return features\n</code></pre>"},{"location":"api/tictactoe/#tictactoe.solve_all_reachable","title":"<code>solve_all_reachable()</code>","text":"<p>Enumerate and solve all states reachable from the empty board.</p> Source code in <code>src/tictactoe/solver.py</code> <pre><code>def solve_all_reachable() -&gt; dict:\n    \"\"\"Enumerate and solve all states reachable from the empty board.\"\"\"\n    all_nodes = {}\n    q = deque()\n    start = tuple([0] * 9)\n    q.append(start)\n    seen = {start}\n    while q:\n        s = q.popleft()\n        all_nodes[s] = True\n        if winner_t(s) != 0 or is_draw_t(s):\n            continue\n        p = current_player_t(s)\n        for mv in legal_moves(s):\n            child = apply_move_t(s, mv, p)\n            if child not in seen:\n                seen.add(child)\n                q.append(child)\n    solved = {}\n    for s in all_nodes:\n        res = solve_state(s)\n        solved[''.join(map(str, s))] = res\n    return solved\n</code></pre>"}]}